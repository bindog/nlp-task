train:
    task_name: "textclf"
    model_name: "nezha"
    pretrained_tag: "huawei/nezha-zh-base"
    pretrained_model: "/path/to/the/pretrained/model"
    batch_size: 48
    train_epochs: 5.0
    freeze_encoder: true
    gradient_checkpointing: true
    fp16: false
    doc_inner_batch_size: 5
    encode_document: False
    output_dir: "/path/to/the/output/dir"  # store wandb logs and trained model files
eval:
    type: "nlu"  # natural language generation
    batch_size: 24
    metric: "precision"
    num_beams: 16
    early_stopping: True
data:
    corpus: "corpus_name"
    data_dir: "/path/to/the/data/dir"
    num_labels: -1
    max_seq_length: 128
    max_tgt_length: 56
    crosslingual: false
optimizer:
    type: "AdamW"
    lr: 0.00005  # mbart 5e-5; for mt5 model, the recommend lr is 5e-4(0.0005) 
    weight_decay: 0.0
    num_warmup_steps: 200
    gradient_accumulation_steps: 1
    max_grad_norm: 1.0
system:
    cuda_devices: "0"  # if num of devices > 1, using data parallel
    distributed: false
