train:
    task_name: "ner"
    model_name: "hfl"
    pretrained_tag: "hfl/chinese-roberta-wwm-ext"
    pretrained_model: "/path/to/the/pretrained/model"
    batch_size: 16
    train_epochs: 10.0
    freeze_encoder: true
    gradient_checkpointing: true
    fp16: false
    encode_document: False
    ner_addBilstm: False  # whether use a model with bilstm added 
    output_dir: "/path/to/the/output/dir"  # store wandb logs and trained model files
eval:
    type: "nlu"  # natural language generation
    metric: "p-r-f1"  # precision, recall and f1
    batch_size: 16
    num_beams: 16
    early_stopping: True
data:
    corpus: "corpus_name"
    data_dir: "/path/to/the/data/dir"
    num_labels: -1
    max_seq_length: 128
    max_tgt_length: 56
    crosslingual: false
optimizer:
    type: "AdamW"
    lr: 0.00005  # mbart 5e-5; for mt5 model, the recommend lr is 5e-4(0.0005) 
    weight_decay: 0.0
    num_warmup_steps: 200
    gradient_accumulation_steps: 1
    max_grad_norm: 1.0
system:
    cuda_devices: "0"  # if num of devices > 1, using data parallel
    distributed: false
