train:
    description: "tag multilabel classification"
    task_name: "tag"
    model_name: "hfl"
    pretrained_tag: "hfl/chinese-roberta-wwm-ext"
    pretrained_model: "/path/to/the/pretrained/model"
    batch_size: 48
    train_epochs: 5.0
    freeze_encoder: true
    gradient_checkpointing: true
    fp16: false
    encode_document: False  # using RNN-like module to encode document after BERT
    doc_inner_batch_size: 5  # if encode_document is True, then this argument decide the inner batch size
    output_dir: "/path/to/the/output/dir"  # store wandb logs and trained model files
eval:
    type: "nlu"  # natural language understanding
    batch_size: 24
    metric: "micro"  # This parameter is required for multilabel tag, ['micro', 'macro']
    early_stopping: True
data:
    corpus: "corpus_name"
    data_dir: "/path/to/the/data/dir"
    num_labels: -1
    max_seq_length: 128
    max_tgt_length: 56
    crosslingual: false
optimizer:
    type: "AdamW"
    lr: 0.00005  # mbart 5e-5; for mt5 model, the recommend lr is 5e-4(0.0005) 
    weight_decay: 0.0
    num_warmup_steps: 200
    gradient_accumulation_steps: 1
    max_grad_norm: 1.0
system:
    cuda_visible_devices: "0"  # if num of devices > 1, using data parallel
    distributed: false
