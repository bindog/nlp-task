train:
    task_name: "translation"
    model_name: "mbart"
    pretrained_tag: "facebook/mbart-large-cc25"
    pretrained_model: "/path/to/the/pretrained/model"
    batch_size: 16
    train_epochs: 10.0
    freeze_encoder: true
    gradient_checkpointing: true
    fp16: false
    output_dir: "/path/to/the/output/dir"  # store wandb logs and trained model files
eval:
    type: "nlg"  # natural language generation
    batch_size: 16
    metric: "bleu"  # blue, rouge, both
    num_beams: 4  # beam search config
    early_stopping: True
data:
    corpus: "corpus_name"
    data_dir: "/path/to/the/data/dir"
    num_labels: -1
    max_src_length: 512
    max_tgt_length: 512
    crosslingual: false
optimizer:
    type: "AdamW"
    lr: 0.00005  # mbart 5e-5; for mt5 model, the recommend lr is 5e-4(0.0005) 
    weight_decay: 0.0
    num_warmup_steps: 200
    gradient_accumulation_steps: 1
    max_grad_norm: 1.0
system:
    cuda_devices: "0"  # if num of devices > 1, using data parallel
    distributed: false
